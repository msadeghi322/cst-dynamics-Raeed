---
title: Prez context signal analysis
subtitle: Taking a look at newly collected CST data
format:
  html:
    toc: true
    toc-location: left
    cap-location: margin
    code-fold: true
    execute: true
    self-contained: true
jupyter: python3
---

# Why we needed more data

Our previous neural data from CO and CST with Earl and Ford suggested something that looked like at least one dimension of neural activity that specified the upcoming trial type (CO or CST), despite behavior being roughly similar in the center hold period across trial types. Because the monkeys knew which type of trial was coming up, this theoretically would allow them to prepare their behavior before movement in whatever way they needed to optimize their success.

Unfortunately, those data had a few problems for this kind of analysis:

1. Monkeys knew what kind of trial was coming up as they were reaching to the center target. This makes it difficult to study the evolution of this contextual preparation signal because it's confounded with a reach.
2. CO was a much shorter task than CST, so it was possible that the signal we saw was merely anticipation of a quicker reward, rather than contextual preparation.
3. CO behavior was much simpler ahead of the reward--only one moment of visual input necessary to specify the behavior for the entire trial in CO, unlike CST, where the monkey has to pay attention to the cursor throughout the trial.
4. The split between CO and CST was not quite even during the main part of the recording session, so there may have been some bias in the monkeys' expectations of upcoming trial.
5. CO involved movements in both horizontal and vertical axes, whereas CST involved movements in only the horizontal axis.

For this reason, we collected new data from another monkey, this time with a few modifications:

1. Instead of CO, we introduced the monkey to a new horizontal random target task (RTT). In the RTT, monkeys would reach to a visually presented target. Once the monkey reached the target, a new reach target would appear for the monkey to reach to. This would continue until the monkey reached to 8 targets sequentially, at which point the trial would end with a reward. Importantly, each of the 8 targets would be selected uniformly from a set of 17 targets lined up on the horizontal axis, so movements would only be horizontal. For this tasks, 8 targets seemed to be a good number to match the 6 second trial time of CST.
2. At the start of the trial, monkeys would be presented with an ambiguous center target to reach and hold in, without knowing which type of trial was coming up. After a short delay (0.3-0.5s), the hold target would change shape to indicate which of the two tasks (CST or RTT) was coming up. After another short delay (0.5-0.75s), the trial would start.

Ideally, these changes would allow for a more careful examination of the putative contextual preparation signal--this notebook serves as an investigation of that.

```{python}
import src
import pyaldata
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import k3d
import yaml
from sklearn.decomposition import PCA
from ipywidgets import interact

sns.set_context('paper')

%load_ext autoreload
%autoreload 2
```

# Data inspection

To start, let's import a dataset (2022/07/20) and preprocess it. This takes a number of preprocessing steps, and they should be somewhat self-evident (if slightly difficult to read) in the code pipeline below.

```{python}
with open('../params.yaml','r') as f:
    params = yaml.safe_load(f)
    inspection_params = params['inspection']

td = (
    pyaldata.mat2dataframe(
        '../data/trial_data/Prez_20220720_RTTCSTCO_TD.mat',
        shift_idx_fields=True,
        td_name='trial_data'
    )
    .assign(
        date_time=lambda x: pd.to_datetime(x['date_time']),
        session_date=lambda x: pd.DatetimeIndex(x['date_time']).normalize()
    )
    .query('task=="RTT" | task=="CST"')
    .pipe(src.data.remove_aborts, verbose=inspection_params['verbose'])
    .pipe(src.data.remove_artifact_trials, verbose=inspection_params['verbose'])
    .pipe(
        src.data.filter_unit_guides,
        filter_func=lambda guide: guide[:,1] > (0 if inspection_params['keep_unsorted'] else 1)
    )
    .pipe(src.data.remove_correlated_units)
    .pipe(
        src.data.remove_all_low_firing_neurons,
        threshold=0.1,
        divide_by_bin_size=True,
        verbose=inspection_params['verbose']
    )
    .pipe(pyaldata.add_firing_rates,method='smooth', std=0.05, backend='convolve')
    .pipe(src.data.trim_nans, ref_signals=['rel_hand_pos'])
    .pipe(src.data.fill_kinematic_signals)
    .pipe(src.data.rebin_data,new_bin_size=inspection_params['bin_size'])
    .pipe(pyaldata.soft_normalize_signal,signals=['M1_rates','PMd_rates','MC_rates'])
    .pipe(pyaldata.dim_reduce,PCA(n_components=15),'M1_rates','M1_pca')
    .pipe(pyaldata.dim_reduce,PCA(n_components=15),'PMd_rates','PMd_pca')
    .pipe(pyaldata.dim_reduce,PCA(n_components=15),'MC_rates','MC_pca')
    .assign(
        **{
            'idx_ctHoldTime': lambda x: x['idx_ctHoldTime'].map(lambda y: y[-1] if y.size>1 else y),
            'Ambiguous Hold Period': lambda x: x['bin_size']*(x['idx_pretaskHoldTime'] - x['idx_ctHoldTime']),
            'Cued Hold Period': lambda x: x['bin_size']*(x['idx_goCueTime'] - x['idx_pretaskHoldTime']),
            'Movement Period': lambda x: x['bin_size']*(x['idx_endTime'] - x['idx_goCueTime']),
        }
    )
)
```

## Individual trials

As a sanity check, let's check out the rasters from a couple random CST trials and RTT trials:

```{python}
_,axs = plt.subplots(4,1,figsize=(6,10),sharex=True)
for ax,trial_id in zip(axs.flatten(),td.groupby('task').sample(n=2)['trial_id']):
    trial = td.loc[td['trial_id']==trial_id,:].squeeze()
    src.plot.make_trial_raster(
        trial,
        ax,
        sig='MC_spikes',
        events=[
            'idx_ctHoldTime',
            'idx_pretaskHoldTime',
            'idx_goCueTime',
            'idx_cstEndTime',
            'idx_rtHoldTimes',
        ],
        ref_event_idx=trial['idx_goCueTime'])
    ax.set_xlabel('')
    ax.set_ylabel(trial['task'])
axs[0].set_title('Neuron Rasters')
axs[-1].set_xlabel('Time (s)')
```

Let's double-check the timing of each trial, just to make sure the task is happening the way we think it is.

```{python}
g = sns.pairplot(
    data=td.query('task=="CST" | task=="RTT"'),
    vars=['Ambiguous Hold Period','Cued Hold Period','Movement Period'],
    hue='task',
    diag_kind='hist',
    corner=True,
    height=1.5,
)
```

The ambiguous and cued hold periods are about the same length for each trial type, and the movement period is slightly shorter for RTT trials than for CST trials, but the trial length disparity is much reduced from that of CO-CST datasets. Perhaps with future data collection, we can increase the number of random targets to more closely match the timing, or we can just have the monkey reaching for random targets until a timer runs out.

## Neural state space in RTT and CST

Now, let's take a look at the individual, smoothed neural traces for each trial type. First CST:

:::{.callout-tip}
These figures below are interactive, so feel free to rotate, zoom and pan to look around.
:::

```{python}
cst_trace_plot = k3d.plot(name='CST smoothed neural traces')
max_abs_hand_vel = np.percentile(np.abs(np.row_stack(td['hand_vel'])[:,0]),95)
# plot traces
for _,trial in td.query('task=="CST"').sample(n=10).iterrows():
    neural_trace = trial['M1_pca']
    cst_trace_plot+=k3d.line(
        neural_trace[:,0:3].astype(np.float32),
        shader='mesh',
        width=3e-3,
        attribute=trial['hand_vel'][:,0],
        color_map=k3d.paraview_color_maps.Erdc_divHi_purpleGreen,
        color_range=[-max_abs_hand_vel,max_abs_hand_vel],
    )
cst_trace_plot.display()
```

Now, let's look at the RTT trials:

```{python}
rtt_trace_plot = k3d.plot(name='RTT smoothed neural traces')
for _,trial in td.query('task=="RTT"').sample(n=10).iterrows():
    neural_trace = trial['M1_pca']
    rtt_trace_plot+=k3d.line(
        neural_trace[:,0:3].astype(np.float32),
        shader='mesh',
        width=3e-3,
        attribute=trial['hand_vel'][:,0],
        color_map=k3d.paraview_color_maps.Erdc_divHi_purpleGreen,
        color_range=[-max_abs_hand_vel,max_abs_hand_vel],
    )
rtt_trace_plot.display()
```

It's tough to really see a whole lot here, but there are two observations:

1. Left and right hand movements are separated in neural state space, with lower velocity movements placed in between the two.
2. RTT neural traces appear slightly more structured--RTT neural traces mostly start in the same place and move up into the cloud of neural activity as the trial continues. In the cloud, there appears to be a sort of "butterfly" of oscillations as the monkey moves left and right. This should be interesting to explore later, as we separate the discrete movements out.

# Looking for the contextual preparatory activity

In previous data, we saw that there was a separation in neural data between CO and CST trials, even before the trial started, and before there was much difference in the behavioral data. Here, we'll look for that same separation between CST and RTT trials, as well as how it evolves through the trial, from ambiguous hold to cued hold to movement. For each of the following analyses, we'll follow the same procedure, but training on different epochs of the data:

1. Extract a 300 ms window of neural data during the epoch of interest (ambiguous hold, cued hold, or movement).
2. Compute the average firing rates of all neurons in this 300 ms bin.
3. Run PCA on the average firing rates across all neurons in this bin.
4. Run LDA on the top 15 principal components of the neural data to find the separability of neural data in this epoch (i.e. ambiguous hold, cued hold, or movement).

We'll also run a similar analysis on the behavior during each of these periods as a control to see if there is any difference in the behavior during the epoch of interest. This follows the same procedure as above, except that instead of PCA, we'll just use the average 3D hand position and velocity during the epoch of interest.

Lastly, we'll also examine the dynamics of this neural separability through the trial. To do this, we'll simply take the smoothed neural data in the trial and run it through the PCA-LDA pipeline that was fit on the epoch of interest. Then, by projecting this smoothed neural data into the found LDA axis, we should see how neural activity evolves over time along this putative context-dependent neural dimension.

:::{.callout-note}
Because the trials have randomized lengths, we'll have to reference these neural dynamics by different time points to get a full picture of the dynamics. In this case, we'll try referencing both to the go cue and to the start of the pre-task cued hold period.
:::

```{python}
# this function extracts epochs from the dataframe and and returns a dataframe with the epochs
# labeled for fitting models and predicting trial type
# Note:The epoch extraction in the code below requires a bit of a kludge to rename the variables,
# but it's a simple way to get the data into the right format for the analysis with the current state of the code.
# TODO: fix the backend code to specify which array we want
td_epoch = src.context_analysis.extract_td_epochs(
    td.rename(columns={'M1_rates':'M1_orig_rates','MC_rates':'M1_rates'})
)
```

## Context in the ambiguous hold period

First, let's take a look at separability in the neural data between CST and RTT trials in the ambiguous hold period. If the task worked out the way we hope it did, there should be little to no separation because the monkey has no idea which trial is coming up. This should serve as a good control for later analyses of this context axis.

```{python}
#| label: ambiguous_hold_separability
#| fig-cap: 
#|   - "Ambiguous Hold Period Neural PC space"
#|   - "Ambiguous Hold Period LDA projection through session"
#|   - "Ambiguous Hold Period Behavior"
#|   - "Ambiguous Hold Period LDA projection through session"
#|   - "Ambiguous Hold Period LDA dynamics (go cue ref)"
#|   - "Ambiguous Hold Period LDA dynamics (trial-type cue ref)"
#|   - "Ambiguous Hold Period LDA dynamics (full trial)"

td_train,td_test_ambig_hold,ambig_hold_lda = src.context_analysis.apply_models(
    td_epoch,
    train_epochs=['ambig_hold'],
    test_epochs=['hold_move','hold_move_ref_cue','full'],
)

fig_gen_dict = {
    'task_M1_pca':src.context_analysis.plot_hold_pca(td_train,array_name='M1',hue_order=['RTT','CST']),
    'task_M1_lda':src.context_analysis.plot_M1_lda(td_train,hue_order=['RTT','CST']),
    'task_beh':src.context_analysis.plot_hold_behavior(td_train,hue_order=['RTT','CST']),
    'task_beh_lda':src.context_analysis.plot_beh_lda(td_train,hue_order=['RTT','CST']),
    # LDA traces
    'task_lda_trace':src.context_analysis.plot_M1_lda_traces(td_test.query('epoch=="hold_move"'),ref_event='idx_goCueTime',label_colors={'RTT':'r','CST':'b'}),
    'task_lda_trace_pretask':src.context_analysis.plot_M1_lda_traces(td_test.query('epoch=="hold_move_ref_cue"'),ref_event='idx_pretaskHoldTime',label_colors={'RTT':'r','CST':'b'}),
    'task_lda_trace':src.context_analysis.plot_M1_lda_traces(td_test.query('epoch=="full"'),ref_event='idx_goCueTime',label_colors={'RTT':'r','CST':'b'}),
}
```

Overall, this shows a few things:

1. The behavior during the ambiguous hold period is pretty similar across the two tasks.
2. The neural data in this period isn't very separable (discriminability 0.61, compared to behavior's 0.59). Note that this is not cross-validated, so we can't expect discriminability to be at or below chance level (0.5) even for fully random data.
3. Projecting neural dynamics into the LDA space defined by the ambiguous hold period doesn't reveal separation between the two tasks randomly (so it's unlikely to randomly find a separation--though there's probably a better control for this).

## Context in the cued hold period

Now we can take a look at contextual separation in the cued hold period, where the monkey finds out which trial is coming up. If there's pre-trial contextual preparatory neural activity, this is where it would show up.

```{python}
#| label: pre_task_hold_separability
#| fig-cap: 
#|   - "Pre-task Hold Period Neural PC space"
#|   - "Pre-task Hold Period LDA projection through session"
#|   - "Pre-task Hold Period Behavior"
#|   - "Pre-task Hold Period LDA projection through session"
#|   - "Pre-task Hold Period LDA dynamics (go cue ref)"
#|   - "Pre-task Hold Period LDA dynamics (trial-type cue ref)"
#|   - "Pre-task Hold Period LDA dynamics (full trial)"
 
td_train,td_test_cued_hold,cued_hold_lda = src.context_analysis.apply_models(
    td_epoch,
    train_epochs=['hold'],
    test_epochs=['hold_move','hold_move_ref_cue','full'],
)

fig_gen_dict = {
    'task_M1_pca':src.context_analysis.plot_hold_pca(td_train,array_name='M1',hue_order=['RTT','CST']),
    'task_M1_lda':src.context_analysis.plot_M1_lda(td_train,hue_order=['RTT','CST']),
    'task_beh':src.context_analysis.plot_hold_behavior(td_train,hue_order=['RTT','CST']),
    'task_beh_lda':src.context_analysis.plot_beh_lda(td_train,hue_order=['RTT','CST']),
    # LDA traces
    'task_lda_trace':src.context_analysis.plot_M1_lda_traces(td_test.query('epoch=="hold_move"'),ref_event='idx_goCueTime',label_colors={'RTT':'r','CST':'b'}),
    'task_lda_trace_pretask':src.context_analysis.plot_M1_lda_traces(td_test.query('epoch=="hold_move_ref_cue"'),ref_event='idx_pretaskHoldTime',label_colors={'RTT':'r','CST':'b'}),
    'task_lda_trace':src.context_analysis.plot_M1_lda_traces(td_test.query('epoch=="full"'),ref_event='idx_goCueTime',label_colors={'RTT':'r','CST':'b'}),
}
```

Again, there are a few things to note from these figures:

1. The behavior during the pre-task hold period is again pretty similar across the two tasks, with fairly low discriminability (0.65, not cross-validated, with a chance level of 0.5).
2. The neural data in this period is quite separable (discriminability 0.8, compared to behavior's 0.65). This suggests that the neural activity separates once the monkey learns which type of trial is coming up, even though the holding behavior remains pretty similar.
3. Projecting neural dynamics into the LDA space defined by the pre-task hold period shows that the separation along this context-dependent neural dimension begins about 300ms after the cued hold period starts and ends about 300ms after the go cue, at which point the neural dynamics along this axis mostly collapse to overlap with each other for the rest of the trial (though there is a slight separation).

## Context in the post-go-cue period

Lastly, for completeness, we run these same analyses on the 300 ms just after the go cue, which should capture the initial bout of movement as the monkey starts to either reach to the first random target (RTT) or control the unstable cursor (CST)

```{python}
#| label: move_separability
#| fig-cap: 
#|   - "Movement Period Neural PC space"
#|   - "Movement Period LDA projection through session"
#|   - "Movement Period Behavior"
#|   - "Movement Period LDA projection through session"
#|   - "Movement Period LDA dynamics (go cue ref)"
#|   - "Movement Period LDA dynamics (trial-type cue ref)"
#|   - "Movement Period LDA dynamics (full trial)"
 
td_train,td_test_move,move_lda = src.context_analysis.apply_models(
    td_epoch,
    train_epochs=['move'],
    test_epochs=['hold_move','hold_move_ref_cue','full'],
)

fig_gen_dict = {
    'task_M1_pca':src.context_analysis.plot_hold_pca(td_train,array_name='M1',hue_order=['RTT','CST']),
    'task_M1_lda':src.context_analysis.plot_M1_lda(td_train,hue_order=['RTT','CST']),
    'task_beh':src.context_analysis.plot_hold_behavior(td_train,hue_order=['RTT','CST']),
    'task_beh_lda':src.context_analysis.plot_beh_lda(td_train,hue_order=['RTT','CST']),
    # 'task_M1_potent': src.plot_M1_hold_potent(td_train,hue_order=['RTT','CST']),
    # 'task_M1_potent_lda': src.plot_M1_potent_lda(td_train,hue_order=['RTT','CST']),
    # 'task_M1_null_lda': src.plot_M1_null_lda(td_train,hue_order=['RTT','CST']),
    # LDA traces
    'task_lda_trace':src.context_analysis.plot_M1_lda_traces(td_test.query('epoch=="hold_move"'),ref_event='idx_goCueTime',label_colors={'RTT':'r','CST':'b'}),
    'task_lda_trace_pretask':src.context_analysis.plot_M1_lda_traces(td_test.query('epoch=="hold_move_ref_cue"'),ref_event='idx_pretaskHoldTime',label_colors={'RTT':'r','CST':'b'}),
    'task_lda_trace':src.context_analysis.plot_M1_lda_traces(td_test.query('epoch=="full"'),ref_event='idx_goCueTime',label_colors={'RTT':'r','CST':'b'}),
}
```

Overall, these plots look fairly similar to the previous (pre-task hold period) figures. The behavior in this epoch still seems similarly distributed between the two tasks, which suggests that the monkey hasn't quite started moving in earnest yet. Also, the neural activity is still pretty separable in this epoch (0.79, compared to behavior's 0.59). However, one striking difference between these figures and the previous set is in the dynamics. Neural activity still separates around 300 ms after the pre-task cued hold period begins, but this separation appears to continue far beyond the go cue into the rest of the trial.

This suggests that there is more than one dimension of neural activity that separate the tasks, with at least one dimension that retains the separation through the entire trial and one (possibly overlapping dimension) in which the separability collapses once the monkey starts to move. This collapsing dimension is reminiscent of a preparatory dimension, while the full-trial separability dimension suggests that behavior occurs in separate regions of the neural state space.

## A more careful look at the dynamics of neural separability

The previous results showed that neural activity may be separable by task throughout the trial, perhaps separating along different dimensions at different times. However, it was a little bit difficult to see the separability in the neural dynamics with all of those traces plotted together. We might be able to get a better picture by looking at how separable the neural activity is along a given dimension in 50 ms bins through the entire trial, referenced to different events in the trial (namely, go cue and pre-task hold cue).

We'll plot out the separabilities over time according to LDA models found from each epoch (ambiguous hold, cued hold, and post-go-cue) As a control, we'll also plot out the separability according to behavior. First, ambiguous hold.

```{python}
#| label: separability_dynamics_ambiguous_hold
#| fig-cap:
#|   - "Separability dynamics according to ambiguous hold **neural** LDA model, aligned by pre-task hold and go cue"
#|   - "Separability dynamics according to ambiguous hold **behavioral** LDA model, aligned by pre-task hold and go cue"
#| 
td_train,td_test = src.context_analysis.apply_models(
    td_epoch,
    train_epochs=['ambig_hold'],
    test_epochs=['full'],
)

fig,axs = plt.subplots(1,2,figsize=(6,2), sharey=True, gridspec_kw={'width_ratios':[1,4]})
src.context_analysis.plot_separability_dynamics(td_test,'Time from pretask hold (s)',ax=axs[0],time_lims=[-0.5,1],pred_name='M1_pred')
src.context_analysis.plot_separability_dynamics(td_test,'Time from go cue (s)',ax=axs[1],time_lims=[-1,5],pred_name='M1_pred')
sns.despine(ax=axs[1],left=True)

fig,axs = plt.subplots(1,2,figsize=(6,2), sharey=True, gridspec_kw={'width_ratios':[1,4]})
src.context_analysis.plot_separability_dynamics(td_test,'Time from pretask hold (s)',ax=axs[0],time_lims=[-0.5,1],pred_name='beh_pred')
src.context_analysis.plot_separability_dynamics(td_test,'Time from go cue (s)',ax=axs[1],time_lims=[-1,5],pred_name='beh_pred')
sns.despine(ax=axs[1],left=True)
```

As expected, not much separability through the whole trial.

Now we'll do the same for the cued hold epoch.

```{python}
#| label: separability_dynamics_cued_hold
#| fig-cap:
#|   - "Separability dynamics according to cued hold **neural** LDA model, aligned by pre-task hold and go cue"
#|   - "Separability dynamics according to cued hold **behavioral** LDA model, aligned by pre-task hold and go cue"
#| 
td_train,td_test = src.context_analysis.apply_models(
    td_epoch,
    train_epochs=['hold'],
    test_epochs=['full'],
)

fig,axs = plt.subplots(1,2,figsize=(6,2), sharey=True, gridspec_kw={'width_ratios':[1,4]})
src.context_analysis.plot_separability_dynamics(td_test,'Time from pretask hold (s)',ax=axs[0],time_lims=[-0.5,1],pred_name='M1_pred')
src.context_analysis.plot_separability_dynamics(td_test,'Time from go cue (s)',ax=axs[1],time_lims=[-1,5],pred_name='M1_pred')
sns.despine(ax=axs[1],left=True)

fig,axs = plt.subplots(1,2,figsize=(6,2), sharey=True, gridspec_kw={'width_ratios':[1,4]})
src.context_analysis.plot_separability_dynamics(td_test,'Time from pretask hold (s)',ax=axs[0],time_lims=[-0.5,1],pred_name='beh_pred')
src.context_analysis.plot_separability_dynamics(td_test,'Time from go cue (s)',ax=axs[1],time_lims=[-1,5],pred_name='beh_pred')
sns.despine(ax=axs[1],left=True)
```

Here, we see that the separability is high through the pre-task hold period, but then drops after the go cue.

Finally, we'll do the same for the post-go-cue epoch.

```{python}
#| label: separability_dynamics_go
#| fig-cap: "Separability dynamics according to post-go-cue LDA model, aligned by pre-task hold and go cue"
#| fig-cap:
#|   - "Separability dynamics according to post-go-cue **neural** LDA model, aligned by pre-task hold and go cue"
#|   - "Separability dynamics according to post-go-cue **behavioral** LDA model, aligned by pre-task hold and go cue"
#| 
td_train,td_test = src.context_analysis.apply_models(
    td_epoch,
    train_epochs=['move'],
    test_epochs=['full'],
)

fig,axs = plt.subplots(1,2,figsize=(6,2), sharey=True, gridspec_kw={'width_ratios':[1,4]})
src.context_analysis.plot_separability_dynamics(td_test,'Time from pretask hold (s)',ax=axs[0],time_lims=[-0.5,1],pred_name='M1_pred')
src.context_analysis.plot_separability_dynamics(td_test,'Time from go cue (s)',ax=axs[1],time_lims=[-1,5],pred_name='M1_pred')
sns.despine(ax=axs[1],left=True)

fig,axs = plt.subplots(1,2,figsize=(6,2), sharey=True, gridspec_kw={'width_ratios':[1,4]})
src.context_analysis.plot_separability_dynamics(td_test,'Time from pretask hold (s)',ax=axs[0],time_lims=[-0.5,1],pred_name='beh_pred')
src.context_analysis.plot_separability_dynamics(td_test,'Time from go cue (s)',ax=axs[1],time_lims=[-1,5],pred_name='beh_pred')
sns.despine(ax=axs[1],left=True)
```

Here, we see high separability that starts in the pre-task hold period and stays high until the end of the trial. Notably, this LDA model was trained with only the first 300 ms after the go cue, and there wasn't much movement during this period yet.

Importantly, the behavioral separability in all of these models remains not much higher than chance level, rising slightly and *briefly* during the periods in which the given LDA model is trained (though not for the ambiguous hold period model). This suggests that the separability in the neural activity is something beyond simply a behavioral difference between the two tasks.

## Any-dimension separability dynamics

The other thing we can do is look at how separable neural activity is along _any_ dimension in the neural state space. This is a bit more involved, since it requires fiting an LDA model at each time point (let's say 50 ms bin) and then looking at the separability of neural activity in the found dimension.

```{python}
#| label: separability_dynamics_any_dim
#| fig-cap: "Separability dynamics along any dimension, aligned by pre-task hold and go cue"
#| fig-cap:
#|   - "Separability dynamics along any neural dimension, aligned by pre-task hold and go cue"
#|   - "Separability dynamics along any behavioral dimension, aligned by pre-task hold and go cue"

array = 'M1'
td_full = (
    td_epoch.copy()
    .loc[td_epoch['epoch']=='full',:]
    .pipe(pyaldata.dim_reduce,PCA(n_components=15),f'{array}_rates',f'{array}_pca')
    .assign(beh_sig=lambda x: x.apply(lambda y: np.column_stack([y['rel_hand_pos'],y['hand_vel']]),axis=1))
)

fig,axs = plt.subplots(1,2,figsize=(6,2), sharey=True, gridspec_kw={'width_ratios':[1,4]})
src.context_analysis.plot_any_dim_separability(
    td_full,
    signal='M1_pca',
    ref_time_col='Time from pretask hold (s)',
    time_lims=[-0.5,1],
    ax=axs[0],
)
src.context_analysis.plot_any_dim_separability(
    td_full,
    signal='M1_pca',
    ref_time_col='Time from go cue (s)',
    time_lims=[-1,5],
    ax=axs[1],
)
sns.despine(ax=axs[1],left=True)

fig,axs = plt.subplots(1,2,figsize=(6,2), sharey=True, gridspec_kw={'width_ratios':[1,4]})
src.context_analysis.plot_any_dim_separability(
    td_full,
    signal='beh_sig',
    ref_time_col='Time from pretask hold (s)',
    time_lims=[-0.5,1],
    ax=axs[0],
)
src.context_analysis.plot_any_dim_separability(
    td_full,
    signal='beh_sig',
    ref_time_col='Time from go cue (s)',
    time_lims=[-1,5],
    ax=axs[1],
)
sns.despine(ax=axs[1],left=True)
```

## How similar are the cued hold and movement period context dimensions?

From previous figures, it seems like there are multiple neural dimensions that separate the two tasks at different times. Open question: how are the two vectors aligned with each other?

Note: we also have to figure out whether these two vectors are significantly different from random, either more aligned or more orthogonal. This means we should also find the distribution of angles between random vectors in the neural space.

```{python}
#| label: angle_between_axes
#| fig-cap: "Angle between cued hold, post-go, and late movement axes (colors) compared to distribution of angles between random pairs of vectors in full neural space (gray)"

def plot_angles(array,ax,label=False):
    dimensionality = td_full[f'{array}_rates'].values[0].shape[1]

    def get_lda_axis(skpipe):
        tester = np.eye(dimensionality)
        intercept = skpipe.get_params()['lda'].intercept_
        return skpipe.transform(tester).squeeze()-intercept

    cued_hold_axis = get_lda_axis(cued_hold_lda[array])
    move_axis = get_lda_axis(move_lda[array])
    move_late_axis = get_lda_axis(move_late_lda[array])
    
    rand_vecs = np.random.randn(100,dimensionality)
    angs = np.array([
        src.util.angle_between(vec1,vec2)
        for vec1 in rand_vecs
        for vec2 in rand_vecs
        if not np.all(vec1==vec2)
    ])

    ax.hist(angs,40,color=[0.8,0.8,0.8])
    ax.bar(src.util.angle_between(cued_hold_axis,move_axis),200,width=3,color='r')
    ax.bar(src.util.angle_between(move_axis,move_late_axis),200,width=3,color='g')
    ax.bar(src.util.angle_between(cued_hold_axis,move_late_axis),200,width=3,color='b')
    if label:
        ax.text(src.util.angle_between(cued_hold_axis,move_axis),100,'Angle between cued hold axis\nand post-go axis',color='r')
        ax.text(src.util.angle_between(move_axis,move_late_axis),200,'Angle between post-go axis\nand late move axis',color='g')
        ax.text(src.util.angle_between(cued_hold_axis,move_late_axis),300,'Angle between cued hold axis\nand late move axis',color='b')
        ax.text(120,0,'Angle between\nrandom pairs of vectors',color=[0.5,0.5,0.5])
    ax.set_ylabel(f"{array}\n({dimensionality} dims)")
    ax.set_xlim([0,180])
    ax.set_yticks([])

fig,ax = plt.subplots(3,1,sharex=True)
plot_angles(array='M1',ax=ax[0],label=True)
plot_angles(array='PMd',ax=ax[1])
plot_angles(array='MC',ax=ax[2])
ax[2].set_xlabel(f"Angle between vectors")
sns.despine(fig=fig,trim=True,left=True)
```

## How many different contextual neural dimensions are there?

By choosing two different epochs, we found two different separability profiles, hinting different contextual dimensions separating the two tasks at different times. A reasonable question might be: how many different dimensions are separating the two tasks in neural activity through time? Relatedly, how many different separability profiles are there? There's at least one with transient separability during the cued hold period and one with separability throughout most of the movement period. It's possible those are the only profiles, but there might be more if we choose different training epochs.

So we're going to look at how much these separability profiles change based on which time bine we train off of.

```{python}
array = 'M1'
td_full = (
    td_epoch.copy()
    .loc[td_epoch['epoch']=='full',:]
    .pipe(pyaldata.dim_reduce,PCA(n_components=15),f'{array}_rates',f'{array}_pca')
    .assign(beh_sig=lambda x: x.apply(lambda y: np.column_stack([y['rel_hand_pos'][:,[0,1,2]],y['hand_vel'][:,:]]),axis=1))
)

M1_sep_profiles,M1_lda_coefs = src.context_analysis.get_train_test_separability(td_full,signal='M1_pca')
# PMd_sep_profiles,PMd_lda_coefs = src.context_analysis.get_train_test_separability(td_full,signal='PMd_pca')
# MC_sep_profiles,MC_lda_coefs = src.context_analysis.get_train_test_separability(td_full,signal='MC_pca')
beh_sep_profiles,_ = src.context_analysis.get_train_test_separability(td_full,signal='beh_sig')

fig,axs = plt.subplots(1,2)
sns.heatmap(
    data = (
        M1_sep_profiles
        .reset_index()
        .assign(**{
            'Time from go cue (s)': lambda x: x['test_time']/np.timedelta64(1,'s'),
            'Training Time (s)': lambda x: x['train_time']/np.timedelta64(1,'s'),
        })
        .pivot('Training Time (s)','Time from go cue (s)','Separability')
    ),
    vmin=0.5,
    vmax=1,
    ax=axs[0],
)
sns.heatmap(
    data = (
        beh_sep_profiles
        .reset_index()
        .assign(**{
            'Time from go cue (s)': lambda x: x['test_time']/np.timedelta64(1,'s'),
            'Training Time (s)': lambda x: x['train_time']/np.timedelta64(1,'s'),
        })
        .pivot('Training Time (s)','Time from go cue (s)','Separability')
    ),
    vmin=0.5,
    vmax=1,
    ax=axs[1],
)

from scipy.spatial.distance import pdist,squareform
def plot_lda_coef_angle(lda_coefs,ax=None):
    coefs = np.row_stack(lda_coefs)
    cos_dist = squareform(pdist(coefs,metric='cosine'))
    
    if ax is None:
        ax = plt.gca()

    sns.heatmap((180/np.pi)*np.arccos(1-cos_dist),vmax=90,vmin=0,ax=ax)
    return ax

ang_fig, ang_ax = plt.subplots(1,1)
ang_ax = plot_lda_coef_angle(M1_lda_coefs)
ang_ax.set_title('Angle between LDA axes trained at different time bins (deg)')
ang_ax.set_xlabel('Time bin')
ang_ax.set_ylabel('Time bin')
```

Seems like there's two main blocks of separability: one transient during the cued hold, and one tonic through the movement period. It also looks like the LDA axes during the cued hold period are pretty much orthogonal to the ones during the thick of movement. Possibly the post-go cue axes are somewhere in between.

At this point, this document is getting a bit unwieldy. I'm going to freeze it here and move some of this stuff over to a new document focusing on these separability metrics. For the purposes of reproducability, here's the git hash of the commit I was on the last time this document was rendered:

```{python}
import git

repo = git.Repo(search_parent_directories=True)
short_hash = repo.head.object.hexsha[:10]
```